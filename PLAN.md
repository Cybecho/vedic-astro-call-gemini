## Go 서비스 구축 계획서: 베다 점성술 AI 해석 서비스

### 프로젝트 개요

본 프로젝트는 베다 점성술 차트 데이터를 입력받아 Google Gemini AI를 통해 해석을 제공하는 Go 기반 웹 서비스를 구축하는 것입니다. 서비스는 먼저 로컬 환경에서 검증한 후 Docker 컨테이너로 배포됩니다.

### 구현 목표

베다 점성술 차트 JSON 데이터를 HTTP 요청으로 받아, 사전 정의된 프롬프트와 함께 Gemini AI에게 전달하여 해석을 생성하고, 그 결과를 클라이언트에게 반환하는 서비스를 만드는 것이 목표입니다. 이를 통해 사용자는 자신의 점성술 차트에 대한 전문적이고 시적인 해석을 받을 수 있습니다.

### 기술 스택 및 요구사항

- **프로그래밍 언어**: Go
- **AI 모델**: Google Gemini 2.5 Flash Lite
- **통신 포트**: 9494
- **프로토콜**: HTTP/REST API
- **배포 방식**: Docker 컨테이너

### 개발 프로세스

#### 1단계: 로컬 개발 환경 구성

먼저 Go 개발 환경을 설정하고 필요한 의존성을 설치합니다. Google Gemini API 클라이언트 라이브러리를 프로젝트에 추가하고, 환경 변수로 API 키를 설정합니다.

#### 2단계: 핵심 기능 구현

HTTP 서버를 구축하여 9494 포트에서 요청을 수신하도록 설정합니다. POST 엔드포인트를 생성하여 JSON 형식의 점성술 차트 데이터를 받을 수 있도록 구현합니다.

받은 데이터는 파일 시스템에서 읽어온 사전 프롬프트(`post-prompt.txt`)와 결합하여 Gemini AI에게 전달됩니다. 이때 프롬프트는 하드코딩하지 않고 파일에서 동적으로 읽어오도록 구현하여 유지보수성을 높입니다.

#### 3단계: 에러 처리 및 응답 구조화

API 호출 실패, 잘못된 입력 데이터, 서버 오류 등 다양한 예외 상황에 대한 처리를 구현합니다. 클라이언트에게 명확한 에러 메시지와 상태 코드를 반환하도록 합니다.

성공적인 응답의 경우, Gemini AI의 해석 결과를 구조화된 JSON 형식으로 반환합니다.

#### 4단계: 로컬 테스트

구현된 서비스를 로컬에서 실행하고, `sample-request.txt`의 데이터를 사용하여 테스트합니다. curl이나 Postman 등의 도구를 사용하여 HTTP 요청을 보내고 응답을 확인합니다.

다양한 시나리오(정상 요청, 잘못된 형식의 요청, 빈 요청 등)에 대한 테스트를 수행하여 서비스의 안정성을 검증합니다.

#### 5단계: Docker 컨테이너화

로컬 테스트가 완료되면 Dockerfile을 작성하여 서비스를 컨테이너화합니다. Go 애플리케이션을 빌드하고, 필요한 파일(`post-prompt.txt`)을 포함하여 최소한의 실행 환경을 구성합니다.

컨테이너 이미지를 빌드하고 9494 포트를 노출하여 실행합니다. 컨테이너 환경에서도 동일하게 작동하는지 확인합니다.

### 데이터 흐름

1. **클라이언트 요청**: 사용자가 베다 점성술 차트 JSON 데이터를 HTTP POST 요청으로 전송
2. **데이터 수신**: Go 서버가 9494 포트에서 요청을 수신하고 JSON 파싱
3. **프롬프트 결합**: 파일에서 읽은 사전 프롬프트와 수신한 데이터를 결합
4. **AI 처리**: Gemini API를 호출하여 해석 생성
5. **응답 반환**: AI의 해석 결과를 클라이언트에게 JSON 형식으로 반환

### 보안 및 성능 고려사항

API 키는 환경 변수로 관리하여 소스 코드에 노출되지 않도록 합니다. 요청 크기 제한을 설정하여 과도한 데이터 전송을 방지하고, 타임아웃을 설정하여 응답 지연 문제를 처리합니다.

동시 요청 처리를 위해 Go의 고루틴을 활용하여 성능을 최적화합니다.

### 문서화

프로젝트 완료 후 다음 문서를 작성합니다:
- **기술 문서**: 구현 방법, 아키텍처, 코드 구조 설명
- **사용자 가이드**: API 엔드포인트, 요청 형식, 응답 형식, 예제 포함
